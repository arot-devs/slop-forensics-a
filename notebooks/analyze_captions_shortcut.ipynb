{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01fae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/yada/dev/slop-forensics-a\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ce8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r /local/yada/dev/slop-forensics-a/requirements.txt  \n",
    "\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330cfcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 02:32:05,624 - INFO - shortcuts - Starting comprehensive slop analysis for 10 sentences\n",
      "2025-07-22 02:32:05,625 - INFO - shortcuts - Creating dataset file...\n",
      "2025-07-22 02:32:05,626 - INFO - shortcuts - Saved dataset to: ./analysis_results/example_1/datasets/generated_sample_text.jsonl\n",
      "2025-07-22 02:32:05,626 - INFO - shortcuts - Performing text analysis...\n",
      "2025-07-22 02:32:05,627 - INFO - analysis - Starting analysis for model: sample_text\n",
      "2025-07-22 02:32:05,627 - WARNING - metrics - NLTK resources ('punkt', 'cmudict') not found. Complexity calculation will be basic.\n",
      "2025-07-22 02:32:05,628 - WARNING - metrics - Run: nltk.download('punkt'); nltk.download('cmudict')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Single Model Analysis ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 02:32:06,217 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic splitting for complexity.\n",
      "2025-07-22 02:32:06,220 - INFO - metrics - Loaded 1000 word items from data/slop_list.json\n",
      "2025-07-22 02:32:06,221 - INFO - metrics - Loaded 200 bigram items from data/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,222 - INFO - metrics - Loaded 200 trigram items from data/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,222 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic regex split for slop index.\n",
      "2025-07-22 02:32:06,223 - WARNING - analysis - No words remained after filtering for sample_text. Skipping rarity and repetition analysis.\n",
      "2025-07-22 02:32:06,224 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,225 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,225 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,226 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,226 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,227 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,228 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,228 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,229 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,229 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,230 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,230 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,231 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,232 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,232 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,233 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,234 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,234 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,235 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,236 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,236 - INFO - analysis - Analysis complete for model: sample_text\n",
      "2025-07-22 02:32:06,236 - INFO - shortcuts - Saved analysis results to: ./analysis_results/example_1/analysis/slop_profile__sample_text.json\n",
      "2025-07-22 02:32:06,237 - INFO - shortcuts - Generating slop lists...\n",
      "2025-07-22 02:32:06,237 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-07-22 02:32:06,237 - INFO - slop_lists - Found 1 analysis files. Loading data...\n",
      "Loading analysis files: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "2025-07-22 02:32:06,239 - INFO - slop_lists - Processing combined text data from 1 models...\n",
      "2025-07-22 02:32:06,240 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|██████████| 10/10 [00:00<00:00, 61052.46it/s]\n",
      "2025-07-22 02:32:06,241 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-07-22 02:32:06,242 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-07-22 02:32:06,336 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-07-22 02:32:06,337 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-07-22 02:32:06,337 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-07-22 02:32:06,338 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_1/slop_lists/slop_list.json\n",
      "2025-07-22 02:32:06,338 - INFO - slop_lists - Saved standard word slop list (17 words).\n",
      "2025-07-22 02:32:06,339 - INFO - slop_lists - Saved frequency-sorted word slop list (17 words).\n",
      "2025-07-22 02:32:06,339 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|██████████| 1/1 [00:00<00:00, 4563.99it/s]\n",
      "2025-07-22 02:32:06,341 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_1/slop_lists/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,341 - INFO - slop_lists - Saved bigram slop list (0 bigrams).\n",
      "2025-07-22 02:32:06,342 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_1/slop_lists/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,342 - INFO - slop_lists - Saved trigram slop list (0 trigrams).\n",
      "2025-07-22 02:32:06,342 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-07-22 02:32:06,342 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-07-22 02:32:06,343 - INFO - slop_lists - Extracting cleaned 3-grams from 10 combined texts...\n",
      "2025-07-22 02:32:06,345 - ERROR - shortcuts - Error generating slop lists: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/shortcuts.py\", line 130, in analyze_sentences\n",
      "    create_slop_lists(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 442, in create_slop_lists\n",
      "    extract_and_save_slop_phrases(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 189, in extract_and_save_slop_phrases\n",
      "    top_ngrams = extract_ngrams_cleaned(texts, n=n, top_k=top_k_ngrams)\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 104, in extract_ngrams_cleaned\n",
      "    for w in word_tokenize(text)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "2025-07-22 02:32:06,345 - INFO - shortcuts - Skipping phylogenetic tree generation\n",
      "2025-07-22 02:32:06,346 - INFO - shortcuts - Generating summary report...\n",
      "2025-07-22 02:32:06,346 - INFO - shortcuts - Analysis complete! Results saved to: ./analysis_results/example_1\n",
      "2025-07-22 02:32:06,347 - INFO - shortcuts - Summary report: ./analysis_results/example_1/analysis_summary.json\n",
      "2025-07-22 02:32:06,347 - INFO - shortcuts - Starting multi-model analysis for 3 models\n",
      "2025-07-22 02:32:06,347 - INFO - shortcuts - Analyzing model: chatgpt_style\n",
      "2025-07-22 02:32:06,348 - INFO - shortcuts - Starting comprehensive slop analysis for 4 sentences\n",
      "2025-07-22 02:32:06,348 - INFO - shortcuts - Creating dataset file...\n",
      "2025-07-22 02:32:06,349 - INFO - shortcuts - Saved dataset to: ./analysis_results/example_2/chatgpt_style/datasets/generated_chatgpt_style.jsonl\n",
      "2025-07-22 02:32:06,349 - INFO - shortcuts - Performing text analysis...\n",
      "2025-07-22 02:32:06,349 - INFO - analysis - Starting analysis for model: chatgpt_style\n",
      "2025-07-22 02:32:06,350 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic splitting for complexity.\n",
      "2025-07-22 02:32:06,351 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic regex split for slop index.\n",
      "2025-07-22 02:32:06,351 - WARNING - analysis - No words remained after filtering for chatgpt_style. Skipping rarity and repetition analysis.\n",
      "2025-07-22 02:32:06,354 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,354 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,355 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,355 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,356 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,357 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,357 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,358 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,358 - INFO - analysis - Analysis complete for model: chatgpt_style\n",
      "2025-07-22 02:32:06,359 - INFO - shortcuts - Saved analysis results to: ./analysis_results/example_2/chatgpt_style/analysis/slop_profile__chatgpt_style.json\n",
      "2025-07-22 02:32:06,359 - INFO - shortcuts - Generating slop lists...\n",
      "2025-07-22 02:32:06,359 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-07-22 02:32:06,360 - INFO - slop_lists - Found 1 analysis files. Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete! Results saved to: ./analysis_results/example_1\n",
      "Summary statistics:\n",
      "  - Average length: 85.4\n",
      "  - Slop score: 55.1181\n",
      "  - Repetitive words found: 0\n",
      "  - Output files: 6 files generated\n",
      "\n",
      "=== Example 2: Multi-Model Analysis ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading analysis files: 100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "2025-07-22 02:32:06,361 - INFO - slop_lists - Processing combined text data from 1 models...\n",
      "2025-07-22 02:32:06,362 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|██████████| 4/4 [00:00<00:00, 37957.50it/s]\n",
      "2025-07-22 02:32:06,363 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-07-22 02:32:06,364 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-07-22 02:32:06,365 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-07-22 02:32:06,365 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-07-22 02:32:06,366 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-07-22 02:32:06,366 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/chatgpt_style/slop_lists/slop_list.json\n",
      "2025-07-22 02:32:06,367 - INFO - slop_lists - Saved standard word slop list (2 words).\n",
      "2025-07-22 02:32:06,367 - INFO - slop_lists - Saved frequency-sorted word slop list (2 words).\n",
      "2025-07-22 02:32:06,367 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|██████████| 1/1 [00:00<00:00, 4262.50it/s]\n",
      "2025-07-22 02:32:06,370 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/chatgpt_style/slop_lists/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,370 - INFO - slop_lists - Saved bigram slop list (0 bigrams).\n",
      "2025-07-22 02:32:06,371 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/chatgpt_style/slop_lists/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,371 - INFO - slop_lists - Saved trigram slop list (0 trigrams).\n",
      "2025-07-22 02:32:06,371 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-07-22 02:32:06,371 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-07-22 02:32:06,372 - INFO - slop_lists - Extracting cleaned 3-grams from 4 combined texts...\n",
      "2025-07-22 02:32:06,373 - ERROR - shortcuts - Error generating slop lists: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/shortcuts.py\", line 130, in analyze_sentences\n",
      "    create_slop_lists(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 442, in create_slop_lists\n",
      "    extract_and_save_slop_phrases(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 189, in extract_and_save_slop_phrases\n",
      "    top_ngrams = extract_ngrams_cleaned(texts, n=n, top_k=top_k_ngrams)\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 104, in extract_ngrams_cleaned\n",
      "    for w in word_tokenize(text)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "2025-07-22 02:32:06,374 - INFO - shortcuts - Skipping phylogenetic tree generation\n",
      "2025-07-22 02:32:06,374 - INFO - shortcuts - Generating summary report...\n",
      "2025-07-22 02:32:06,375 - INFO - shortcuts - Analysis complete! Results saved to: ./analysis_results/example_2/chatgpt_style\n",
      "2025-07-22 02:32:06,375 - INFO - shortcuts - Summary report: ./analysis_results/example_2/chatgpt_style/analysis_summary.json\n",
      "2025-07-22 02:32:06,376 - INFO - shortcuts - Analyzing model: academic_style\n",
      "2025-07-22 02:32:06,376 - INFO - shortcuts - Starting comprehensive slop analysis for 4 sentences\n",
      "2025-07-22 02:32:06,376 - INFO - shortcuts - Creating dataset file...\n",
      "2025-07-22 02:32:06,377 - INFO - shortcuts - Saved dataset to: ./analysis_results/example_2/academic_style/datasets/generated_academic_style.jsonl\n",
      "2025-07-22 02:32:06,377 - INFO - shortcuts - Performing text analysis...\n",
      "2025-07-22 02:32:06,377 - INFO - analysis - Starting analysis for model: academic_style\n",
      "2025-07-22 02:32:06,378 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic splitting for complexity.\n",
      "2025-07-22 02:32:06,379 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic regex split for slop index.\n",
      "2025-07-22 02:32:06,379 - WARNING - analysis - No words remained after filtering for academic_style. Skipping rarity and repetition analysis.\n",
      "2025-07-22 02:32:06,380 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,381 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,381 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,382 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,382 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,383 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,384 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,384 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,384 - INFO - analysis - Analysis complete for model: academic_style\n",
      "2025-07-22 02:32:06,385 - INFO - shortcuts - Saved analysis results to: ./analysis_results/example_2/academic_style/analysis/slop_profile__academic_style.json\n",
      "2025-07-22 02:32:06,385 - INFO - shortcuts - Generating slop lists...\n",
      "2025-07-22 02:32:06,386 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-07-22 02:32:06,386 - INFO - slop_lists - Found 1 analysis files. Loading data...\n",
      "Loading analysis files: 100%|██████████| 1/1 [00:00<00:00, 6689.48it/s]\n",
      "2025-07-22 02:32:06,388 - INFO - slop_lists - Processing combined text data from 1 models...\n",
      "2025-07-22 02:32:06,388 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|██████████| 4/4 [00:00<00:00, 39475.80it/s]\n",
      "2025-07-22 02:32:06,389 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-07-22 02:32:06,390 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-07-22 02:32:06,391 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-07-22 02:32:06,391 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-07-22 02:32:06,392 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-07-22 02:32:06,392 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/academic_style/slop_lists/slop_list.json\n",
      "2025-07-22 02:32:06,393 - INFO - slop_lists - Saved standard word slop list (5 words).\n",
      "2025-07-22 02:32:06,393 - INFO - slop_lists - Saved frequency-sorted word slop list (5 words).\n",
      "2025-07-22 02:32:06,393 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|██████████| 1/1 [00:00<00:00, 10058.28it/s]\n",
      "2025-07-22 02:32:06,395 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/academic_style/slop_lists/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,395 - INFO - slop_lists - Saved bigram slop list (0 bigrams).\n",
      "2025-07-22 02:32:06,395 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/academic_style/slop_lists/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,396 - INFO - slop_lists - Saved trigram slop list (0 trigrams).\n",
      "2025-07-22 02:32:06,396 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-07-22 02:32:06,396 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-07-22 02:32:06,396 - INFO - slop_lists - Extracting cleaned 3-grams from 4 combined texts...\n",
      "2025-07-22 02:32:06,398 - ERROR - shortcuts - Error generating slop lists: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/shortcuts.py\", line 130, in analyze_sentences\n",
      "    create_slop_lists(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 442, in create_slop_lists\n",
      "    extract_and_save_slop_phrases(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 189, in extract_and_save_slop_phrases\n",
      "    top_ngrams = extract_ngrams_cleaned(texts, n=n, top_k=top_k_ngrams)\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 104, in extract_ngrams_cleaned\n",
      "    for w in word_tokenize(text)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "2025-07-22 02:32:06,399 - INFO - shortcuts - Skipping phylogenetic tree generation\n",
      "2025-07-22 02:32:06,399 - INFO - shortcuts - Generating summary report...\n",
      "2025-07-22 02:32:06,400 - INFO - shortcuts - Analysis complete! Results saved to: ./analysis_results/example_2/academic_style\n",
      "2025-07-22 02:32:06,400 - INFO - shortcuts - Summary report: ./analysis_results/example_2/academic_style/analysis_summary.json\n",
      "2025-07-22 02:32:06,400 - INFO - shortcuts - Analyzing model: creative_writing\n",
      "2025-07-22 02:32:06,401 - INFO - shortcuts - Starting comprehensive slop analysis for 4 sentences\n",
      "2025-07-22 02:32:06,401 - INFO - shortcuts - Creating dataset file...\n",
      "2025-07-22 02:32:06,402 - INFO - shortcuts - Saved dataset to: ./analysis_results/example_2/creative_writing/datasets/generated_creative_writing.jsonl\n",
      "2025-07-22 02:32:06,402 - INFO - shortcuts - Performing text analysis...\n",
      "2025-07-22 02:32:06,402 - INFO - analysis - Starting analysis for model: creative_writing\n",
      "2025-07-22 02:32:06,403 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic splitting for complexity.\n",
      "2025-07-22 02:32:06,404 - WARNING - metrics - NLTK 'punkt' tokenizer not found. Using basic regex split for slop index.\n",
      "2025-07-22 02:32:06,404 - WARNING - analysis - No words remained after filtering for creative_writing. Skipping rarity and repetition analysis.\n",
      "2025-07-22 02:32:06,405 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,405 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,406 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,406 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,407 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,408 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,408 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,409 - WARNING - analysis - NLTK 'punkt' tokenizer not found. Using basic split for ngrams.\n",
      "2025-07-22 02:32:06,409 - INFO - analysis - Analysis complete for model: creative_writing\n",
      "2025-07-22 02:32:06,410 - INFO - shortcuts - Saved analysis results to: ./analysis_results/example_2/creative_writing/analysis/slop_profile__creative_writing.json\n",
      "2025-07-22 02:32:06,410 - INFO - shortcuts - Generating slop lists...\n",
      "2025-07-22 02:32:06,410 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-07-22 02:32:06,411 - INFO - slop_lists - Found 1 analysis files. Loading data...\n",
      "Loading analysis files: 100%|██████████| 1/1 [00:00<00:00, 3919.91it/s]\n",
      "2025-07-22 02:32:06,412 - INFO - slop_lists - Processing combined text data from 1 models...\n",
      "2025-07-22 02:32:06,412 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|██████████| 4/4 [00:00<00:00, 41734.37it/s]\n",
      "2025-07-22 02:32:06,414 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-07-22 02:32:06,414 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-07-22 02:32:06,415 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-07-22 02:32:06,416 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-07-22 02:32:06,416 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-07-22 02:32:06,416 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/creative_writing/slop_lists/slop_list.json\n",
      "2025-07-22 02:32:06,417 - INFO - slop_lists - Saved standard word slop list (6 words).\n",
      "2025-07-22 02:32:06,417 - INFO - slop_lists - Saved frequency-sorted word slop list (6 words).\n",
      "2025-07-22 02:32:06,417 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|██████████| 1/1 [00:00<00:00, 5065.58it/s]\n",
      "2025-07-22 02:32:06,419 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/creative_writing/slop_lists/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,419 - INFO - slop_lists - Saved bigram slop list (0 bigrams).\n",
      "2025-07-22 02:32:06,420 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/creative_writing/slop_lists/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,420 - INFO - slop_lists - Saved trigram slop list (0 trigrams).\n",
      "2025-07-22 02:32:06,420 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-07-22 02:32:06,421 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-07-22 02:32:06,421 - INFO - slop_lists - Extracting cleaned 3-grams from 4 combined texts...\n",
      "2025-07-22 02:32:06,422 - ERROR - shortcuts - Error generating slop lists: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/shortcuts.py\", line 130, in analyze_sentences\n",
      "    create_slop_lists(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 442, in create_slop_lists\n",
      "    extract_and_save_slop_phrases(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 189, in extract_and_save_slop_phrases\n",
      "    top_ngrams = extract_ngrams_cleaned(texts, n=n, top_k=top_k_ngrams)\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 104, in extract_ngrams_cleaned\n",
      "    for w in word_tokenize(text)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "2025-07-22 02:32:06,423 - INFO - shortcuts - Skipping phylogenetic tree generation\n",
      "2025-07-22 02:32:06,423 - INFO - shortcuts - Generating summary report...\n",
      "2025-07-22 02:32:06,424 - INFO - shortcuts - Analysis complete! Results saved to: ./analysis_results/example_2/creative_writing\n",
      "2025-07-22 02:32:06,424 - INFO - shortcuts - Summary report: ./analysis_results/example_2/creative_writing/analysis_summary.json\n",
      "2025-07-22 02:32:06,425 - INFO - shortcuts - Generating combined slop lists...\n",
      "2025-07-22 02:32:06,425 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-07-22 02:32:06,426 - INFO - slop_lists - Found 3 analysis files. Loading data...\n",
      "Loading analysis files: 100%|██████████| 3/3 [00:00<00:00, 7427.93it/s]\n",
      "2025-07-22 02:32:06,427 - INFO - slop_lists - Processing combined text data from 3 models...\n",
      "2025-07-22 02:32:06,428 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|██████████| 12/12 [00:00<00:00, 66313.11it/s]\n",
      "2025-07-22 02:32:06,429 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-07-22 02:32:06,430 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-07-22 02:32:06,431 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-07-22 02:32:06,431 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-07-22 02:32:06,432 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-07-22 02:32:06,432 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/combined_slop_lists/slop_list.json\n",
      "2025-07-22 02:32:06,432 - INFO - slop_lists - Saved standard word slop list (13 words).\n",
      "2025-07-22 02:32:06,433 - INFO - slop_lists - Saved frequency-sorted word slop list (13 words).\n",
      "2025-07-22 02:32:06,433 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|██████████| 3/3 [00:00<00:00, 14961.85it/s]\n",
      "2025-07-22 02:32:06,435 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/combined_slop_lists/slop_list_bigrams.json\n",
      "2025-07-22 02:32:06,436 - INFO - slop_lists - Saved bigram slop list (0 bigrams).\n",
      "2025-07-22 02:32:06,436 - INFO - utils - Saved list with one item per line to: ./analysis_results/example_2/combined_slop_lists/slop_list_trigrams.json\n",
      "2025-07-22 02:32:06,436 - INFO - slop_lists - Saved trigram slop list (0 trigrams).\n",
      "2025-07-22 02:32:06,437 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-07-22 02:32:06,437 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-07-22 02:32:06,437 - INFO - slop_lists - Extracting cleaned 3-grams from 12 combined texts...\n",
      "2025-07-22 02:32:06,439 - ERROR - shortcuts - Error generating combined slop lists: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/shortcuts.py\", line 316, in analyze_multiple_models\n",
      "    create_slop_lists(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 442, in create_slop_lists\n",
      "    extract_and_save_slop_phrases(\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 189, in extract_and_save_slop_phrases\n",
      "    top_ngrams = extract_ngrams_cleaned(texts, n=n, top_k=top_k_ngrams)\n",
      "  File \"/local/yada/dev/slop-forensics-a/slop_forensics/slop_lists.py\", line 104, in extract_ngrams_cleaned\n",
      "    for w in word_tokenize(text)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/root/nltk_data'\n",
      "    - '/root/miniconda3/nltk_data'\n",
      "    - '/root/miniconda3/share/nltk_data'\n",
      "    - '/root/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "2025-07-22 02:32:06,440 - INFO - shortcuts - Generating phylogenetic trees for multiple models...\n",
      "2025-07-22 02:32:06,440 - INFO - phylogeny - Starting phylogenetic tree generation...\n",
      "2025-07-22 02:32:06,440 - INFO - phylogeny - Loading combined metrics data from: ./analysis_results/example_2/combined_metrics.json\n",
      "2025-07-22 02:32:06,441 - INFO - phylogeny - Extracting features (top words/ngrams) for tree building...\n",
      "2025-07-22 02:32:06,441 - WARNING - phylogeny - No features extracted for model: chatgpt_style\n",
      "2025-07-22 02:32:06,441 - WARNING - phylogeny - No features extracted for model: academic_style\n",
      "2025-07-22 02:32:06,442 - WARNING - phylogeny - No features extracted for model: creative_writing\n",
      "2025-07-22 02:32:06,442 - ERROR - phylogeny - Need at least 2 models with features to build a tree. Found 0. Stopping.\n",
      "2025-07-22 02:32:06,443 - INFO - shortcuts - Multi-model analysis complete! Results saved to: ./analysis_results/example_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-model analysis complete!\n",
      "Models analyzed: ['chatgpt_style', 'academic_style', 'creative_writing']\n",
      "Total sentences: 12\n",
      "Results saved to: ./analysis_results/example_2\n",
      "\n",
      "=== Analysis Complete ===\n",
      "Check the output directories for detailed results:\n",
      "  - analysis_summary.json: Human-readable summary\n",
      "  - slop_lists/: Generated slop word lists\n",
      "  - phylogeny/: Phylogenetic tree visualizations (if generated)\n",
      "  - analysis/: Detailed analysis metrics\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Example usage of the slop-forensics shortcuts module.\n",
    "This demonstrates how to use the unified analyze_sentences function.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "# project_root = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(0, project_root)\n",
    "\n",
    "from slop_forensics.shortcuts import analyze_sentences, analyze_multiple_models\n",
    "\n",
    "def main():\n",
    "    # Example 1: Analyze a single set of sentences\n",
    "    print(\"=== Example 1: Single Model Analysis ===\")\n",
    "    \n",
    "    # Sample sentences that might contain \"slop\" (repetitive AI-generated text patterns)\n",
    "    sample_sentences = [\n",
    "        \"The sun was setting over the horizon, casting a warm glow across the landscape.\",\n",
    "        \"In conclusion, it's important to note that this matter requires careful consideration.\",\n",
    "        \"The atmosphere was filled with palpable tension as the characters navigated their journey.\",\n",
    "        \"It's worth mentioning that the situation demanded immediate attention and careful analysis.\",\n",
    "        \"The protagonist found themselves in a precarious situation that would test their resolve.\",\n",
    "        \"Furthermore, the implications of this decision would reverberate throughout the narrative.\",\n",
    "        \"The complex web of relationships added layers of depth to the unfolding drama.\",\n",
    "        \"As the story progressed, it became increasingly clear that the stakes were higher than anticipated.\",\n",
    "        \"The intricate plot threads began to weave together in unexpected ways.\",\n",
    "        \"Ultimately, the resolution provided a satisfying conclusion to the elaborate tale.\"\n",
    "    ]\n",
    "    \n",
    "    # Analyze the sentences\n",
    "    results = analyze_sentences(\n",
    "        sentences=sample_sentences,\n",
    "        output_dir=\"./analysis_results/example_1\",\n",
    "        model_name=\"sample_text\",\n",
    "        generate_phylogeny=False  # Skip phylogeny for single model\n",
    "    )\n",
    "    \n",
    "    print(f\"Analysis complete! Results saved to: ./analysis_results/example_1\")\n",
    "    print(f\"Summary statistics:\")\n",
    "    print(f\"  - Average length: {results['statistics'].get('avg_length', 0)}\")\n",
    "    print(f\"  - Slop score: {results['statistics'].get('slop_score', 0)}\")\n",
    "    print(f\"  - Repetitive words found: {results['statistics'].get('num_repetitive_words', 0)}\")\n",
    "    print(f\"  - Output files: {len(results['output_paths'])} files generated\")\n",
    "    print()\n",
    "    \n",
    "    # Example 2: Analyze multiple models/sources\n",
    "    print(\"=== Example 2: Multi-Model Analysis ===\")\n",
    "    \n",
    "    # Sample data from different \"models\" or sources\n",
    "    model_data = {\n",
    "        \"chatgpt_style\": [\n",
    "            \"I'd be happy to help you with that! Here's what you need to know about this topic.\",\n",
    "            \"It's important to note that there are several key considerations to keep in mind.\",\n",
    "            \"In summary, the best approach would be to carefully evaluate your options.\",\n",
    "            \"I hope this information helps! Let me know if you have any other questions.\",\n",
    "        ],\n",
    "        \"academic_style\": [\n",
    "            \"This research demonstrates significant implications for future studies in the field.\",\n",
    "            \"The methodology employed in this investigation follows established protocols.\",\n",
    "            \"Furthermore, the results indicate a strong correlation between the variables.\",\n",
    "            \"In conclusion, these findings contribute to our understanding of the phenomenon.\",\n",
    "        ],\n",
    "        \"creative_writing\": [\n",
    "            \"The moonlight danced across the rippling water, creating patterns of silver and shadow.\",\n",
    "            \"Her heart pounded with anticipation as she approached the mysterious door.\",\n",
    "            \"The ancient forest whispered secrets that only the wind could understand.\",\n",
    "            \"Time seemed to stand still in that magical moment of discovery.\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Analyze multiple models\n",
    "    multi_results = analyze_multiple_models(\n",
    "        model_sentences=model_data,\n",
    "        output_dir=\"./analysis_results/example_2\",\n",
    "        generate_phylogeny=True  # Generate phylogeny for multiple models\n",
    "    )\n",
    "    \n",
    "    print(f\"Multi-model analysis complete!\")\n",
    "    print(f\"Models analyzed: {multi_results['models_analyzed']}\")\n",
    "    print(f\"Total sentences: {multi_results['total_sentences']}\")\n",
    "    print(f\"Results saved to: ./analysis_results/example_2\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Analysis Complete ===\")\n",
    "    print(\"Check the output directories for detailed results:\")\n",
    "    print(\"  - analysis_summary.json: Human-readable summary\")\n",
    "    print(\"  - slop_lists/: Generated slop word lists\")\n",
    "    print(\"  - phylogeny/: Phylogenetic tree visualizations (if generated)\")\n",
    "    print(\"  - analysis/: Detailed analysis metrics\")\n",
    "\n",
    "\n",
    "def quick_analyze(sentences, output_dir=\"./quick_analysis\"):\n",
    "    \"\"\"Quick analysis function for simple use cases.\"\"\"\n",
    "    results = analyze_sentences(\n",
    "        sentences=sentences,\n",
    "        output_dir=output_dir,\n",
    "        model_name=\"quick_analysis\",\n",
    "        generate_phylogeny=False\n",
    "    )\n",
    "    \n",
    "    # Print quick summary\n",
    "    print(\"Quick Analysis Results:\")\n",
    "    print(f\"  Sentences analyzed: {len(sentences)}\")\n",
    "    print(f\"  Average length: {results['statistics'].get('avg_length', 0):.1f} characters\")\n",
    "    print(f\"  Slop score: {results['statistics'].get('slop_score', 0):.3f}\")\n",
    "    print(f\"  Repetitive words: {results['statistics'].get('num_repetitive_words', 0)}\")\n",
    "    \n",
    "    # Show top repetitive words if any\n",
    "    analysis_file = results[\"output_paths\"].get(\"analysis\")\n",
    "    if analysis_file and os.path.exists(analysis_file):\n",
    "        import json\n",
    "        with open(analysis_file, 'r') as f:\n",
    "            analysis_data = json.load(f)\n",
    "        \n",
    "        top_words = analysis_data.get(\"top_repetitive_words\", [])[:5]\n",
    "        if top_words:\n",
    "            print(\"  Top repetitive words:\")\n",
    "            for word_data in top_words:\n",
    "                word = word_data.get(\"word\", \"\")\n",
    "                score = word_data.get(\"score\", 0)\n",
    "                print(f\"    - '{word}' (score: {score:.2f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee007d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
