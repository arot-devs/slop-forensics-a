{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9eb661",
   "metadata": {},
   "source": [
    "processing part:\n",
    "\n",
    "this is done on another machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This avoids auto-generating a split — returns a DatasetDict\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# --- Configuration ---\n",
    "# dataset = load_dataset(\"incantor/danbooru_mongodb-quail_dump-feb-cap-only\", split=None, num_proc=16)\n",
    "# dataset = dataset['train']\n",
    "\n",
    "sample_size = 500_000\n",
    "seed = 42  # for reproducibility\n",
    "\n",
    "caption_columns = [col for col in dataset.column_names if col.startswith(\"auto-caption__\")]\n",
    "print(f\"Caption columns: {caption_columns}\")\n",
    "output_directory = \"qa_style_jsons\"\n",
    "prompt_source_name = \"auto_caption_prompts\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# --- Helper: Sanitize filename ---\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    sanitized = name.replace(\"/\", \"__\")\n",
    "    sanitized = re.sub(r'[<>:\"|?*\\\\ ]', '-', sanitized)\n",
    "    sanitized = sanitized.strip('-_')\n",
    "    return sanitized if sanitized else \"invalid_name\"\n",
    "\n",
    "# --- Sample 0.5M examples ---\n",
    "print(f\"Sampling {sample_size:,} examples...\")\n",
    "sampled_dataset = dataset.shuffle(seed=seed).select(range(sample_size))\n",
    "\n",
    "# --- Process each caption column ---\n",
    "for caption_column in caption_columns:\n",
    "    print(f\"Processing caption column: {caption_column}\")\n",
    "    sanitized_model_name = sanitize_filename(caption_column)\n",
    "    output_filename = os.path.join(output_directory, f\"generated_{sanitized_model_name}.jsonl\")\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for i, example in enumerate(sampled_dataset):\n",
    "            raw_caption = example[caption_column]\n",
    "            caption = raw_caption.strip() if isinstance(raw_caption, str) else \"\"\n",
    "            if not caption:\n",
    "                continue  # skip empty or missing captions\n",
    "\n",
    "            record = {\n",
    "                \"source\": prompt_source_name,\n",
    "                \"id\": str(i),\n",
    "                \"prompt\": f\"provide description for the image: {example['filepath']}\",\n",
    "                \"model\": caption_column,\n",
    "                \"output\": caption\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"  Wrote JSONL with sampled data to: {output_filename}\")\n",
    "\n",
    "print(\"\\nDone creating QA-style JSONL files from sampled subset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync qa_style_jsons s3://dataset-ingested/temp/captions-analyze-samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef1e61",
   "metadata": {},
   "source": [
    "since the whole file is too big to handle, it is processed in another machine and doing analysis only here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbdd3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataset-ingested/temp/captions-analyze-samples/generated_auto-caption__captioner-1.4__200words.jsonl to data/captions-analyze-samples/generated_auto-caption__captioner-1.4__200words.jsonl\n",
      "download: s3://dataset-ingested/temp/captions-analyze-samples/generated_auto-caption__captioner-1.45__200words.jsonl to data/captions-analyze-samples/generated_auto-caption__captioner-1.45__200words.jsonl\n",
      "download: s3://dataset-ingested/temp/captions-analyze-samples/generated_auto-caption__captioner-1.45__full.jsonl to data/captions-analyze-samples/generated_auto-caption__captioner-1.45__full.jsonl\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataset-ingested/temp/captions-analyze-samples/ data/captions-analyze-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13025b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY not found in environment variables or .env file.\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Starting analysis of datasets in: data/captions-analyze-samples\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Analysis output directory: /home/ubuntu/dev/slop-forensics-a/results/analysis\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Combined metrics output file: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Max items per model: 10000\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Will log top 5 patterns per model\n",
      "2025-04-14 14:36:44,140 - INFO - slop_profile - Found 3 dataset files to analyze.\n",
      "Analyzing Models:   0%|                                   | 0/3 [00:00<?, ?it/s]2025-04-14 14:36:44,142 - INFO - slop_profile - Processing file: generated_auto-caption__captioner-1.4__200words.jsonl\n",
      "2025-04-14 14:36:44,189 - INFO - utils - Reached max_items limit (10000) for data/captions-analyze-samples/generated_auto-caption__captioner-1.4__200words.jsonl.\n",
      "2025-04-14 14:36:44,189 - INFO - slop_profile - Analyzing model: auto-caption__captioner-1.4__200words (10000 items)\n",
      "2025-04-14 14:36:44,198 - INFO - analysis - Starting analysis for model: auto-caption__captioner-1.4__200words\n",
      "2025-04-14 14:36:55,490 - WARNING - metrics - Slop file for type 'word' not found at data/slop_list.json. Returning empty set.\n",
      "2025-04-14 14:36:55,490 - WARNING - metrics - Slop file for type 'bigram' not found at data/slop_list_bigrams.json. Returning empty set.\n",
      "2025-04-14 14:36:55,490 - WARNING - metrics - Slop file for type 'trigram' not found at data/slop_list_trigrams.json. Returning empty set.\n",
      "2025-04-14 14:36:55,490 - WARNING - metrics - No slop lists loaded. Returning slop index 0.\n",
      "2025-04-14 14:37:14,066 - INFO - analysis - Analysis complete for model: auto-caption__captioner-1.4__200words\n",
      "Analyzing Models:  33%|█████████                  | 1/3 [00:29<00:59, 29.95s/it]2025-04-14 14:37:14,090 - INFO - slop_profile - Processing file: generated_auto-caption__captioner-1.45__200words.jsonl\n",
      "2025-04-14 14:37:14,140 - INFO - utils - Reached max_items limit (10000) for data/captions-analyze-samples/generated_auto-caption__captioner-1.45__200words.jsonl.\n",
      "2025-04-14 14:37:14,142 - INFO - slop_profile - Analyzing model: auto-caption__captioner-1.45__200words (10000 items)\n",
      "2025-04-14 14:37:14,153 - INFO - analysis - Starting analysis for model: auto-caption__captioner-1.45__200words\n",
      "2025-04-14 14:37:25,796 - WARNING - metrics - No slop lists loaded. Returning slop index 0.\n",
      "2025-04-14 14:37:47,015 - INFO - analysis - Analysis complete for model: auto-caption__captioner-1.45__200words\n",
      "Analyzing Models:  67%|██████████████████         | 2/3 [01:02<00:31, 31.71s/it]2025-04-14 14:37:47,041 - INFO - slop_profile - Processing file: generated_auto-caption__captioner-1.45__full.jsonl\n",
      "2025-04-14 14:37:47,147 - INFO - utils - Reached max_items limit (10000) for data/captions-analyze-samples/generated_auto-caption__captioner-1.45__full.jsonl.\n",
      "2025-04-14 14:37:47,149 - INFO - slop_profile - Analyzing model: auto-caption__captioner-1.45__full (10000 items)\n",
      "2025-04-14 14:37:47,287 - INFO - analysis - Starting analysis for model: auto-caption__captioner-1.45__full\n",
      "2025-04-14 14:38:24,872 - WARNING - metrics - No slop lists loaded. Returning slop index 0.\n",
      "2025-04-14 14:39:34,385 - INFO - analysis - Analysis complete for model: auto-caption__captioner-1.45__full\n",
      "Analyzing Models: 100%|███████████████████████████| 3/3 [02:50<00:00, 56.76s/it]\n",
      "2025-04-14 14:39:34,436 - INFO - slop_profile - Saving combined metrics for 3 models to /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - \n",
      "========== SUMMARY OF TOP PATTERNS ==========\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - MODEL: auto-caption__captioner-1.4__200words\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - WORDS: 'thighhighs', 'serafuku', 'sweatdrop', 'wariza', 'arknights'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - BIGRAMS: 'style image', 'background simple', 'hair styled', 'image drawn', 'directly viewer'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - TRIGRAMS: 'looking directly viewer', 'background simple white', 'style image typical', 'image typical anime', 'typical anime art'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - ---\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - MODEL: auto-caption__captioner-1.45__200words\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - WORDS: 'thighhighs', 'serafuku', 'sweatdrop', 'arknights', 'wariza'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - BIGRAMS: 'hair styled', 'style image', 'background simple', 'image rendered', 'right hand'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - TRIGRAMS: 'background simple white', 'looking directly viewer', 'style image typical', 'lines vibrant colors', 'image typical anime'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - ---\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - MODEL: auto-caption__captioner-1.45__full\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - WORDS: 'thighhighs', 'serafuku', 'arknights', 'sweatdrop', 'mokou'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - BIGRAMS: 'artist intent', 'intent appears', 'hair styled', 'style image', 'right hand'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - TRIGRAMS: 'artist intent appears', 'looking directly viewer', 'artist intent seems', 'intent appears capture', 'image detailed illustration'\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - ---\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - ============== END SUMMARY ===============\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - Analysis script finished.\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - \n",
      "Full results are available at:\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - - Combined metrics file: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 14:39:34,466 - INFO - slop_profile - - Individual analysis files: /home/ubuntu/dev/slop-forensics-a/results/analysis/analysis_*.json\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/slop_profile.py --input-dir data/captions-analyze-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7edef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY not found in environment variables or .env file.\n",
      "2025-04-14 14:42:36,838 - INFO - create_slop_lists - Starting slop list creation from analysis files in: /home/ubuntu/dev/slop-forensics-a/results/analysis\n",
      "2025-04-14 14:42:36,838 - INFO - create_slop_lists - Output directory: data/captions-analyze-samples-outputs\n",
      "2025-04-14 14:42:36,838 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-04-14 14:42:36,838 - INFO - slop_lists - Found 3 analysis files. Loading data...\n",
      "Loading analysis files:   0%|                             | 0/3 [00:00<?, ?it/s]2025-04-14 14:42:36,964 - INFO - utils - Reached max_items limit (10000) for /home/ubuntu/dev/slop-forensics-a/results/datasets/generated_auto-caption__captioner-1.45__full.jsonl.\n",
      "Loading analysis files:  33%|███████              | 1/3 [00:00<00:00,  7.78it/s]2025-04-14 14:42:37,016 - INFO - utils - Reached max_items limit (10000) for /home/ubuntu/dev/slop-forensics-a/results/datasets/generated_auto-caption__captioner-1.4__200words.jsonl.\n",
      "2025-04-14 14:42:37,078 - INFO - utils - Reached max_items limit (10000) for /home/ubuntu/dev/slop-forensics-a/results/datasets/generated_auto-caption__captioner-1.45__200words.jsonl.\n",
      "Loading analysis files: 100%|█████████████████████| 3/3 [00:00<00:00, 12.26it/s]\n",
      "2025-04-14 14:42:37,084 - INFO - slop_lists - Processing combined text data from 3 models...\n",
      "2025-04-14 14:42:37,085 - INFO - slop_lists - Counting combined words...\n",
      "Counting words: 100%|███████████████████| 30000/30000 [00:04<00:00, 6844.62it/s]\n",
      "2025-04-14 14:42:41,468 - INFO - slop_lists - Filtering combined counts...\n",
      "2025-04-14 14:42:41,506 - INFO - slop_lists - Analyzing combined word rarity...\n",
      "2025-04-14 14:42:41,846 - INFO - slop_lists - Filtering common words (wordfreq > 1.2e-05)...\n",
      "2025-04-14 14:42:41,853 - INFO - slop_lists - Finding over-represented and zero-frequency words...\n",
      "2025-04-14 14:42:41,874 - INFO - slop_lists - Creating final word slop lists...\n",
      "2025-04-14 14:42:41,878 - INFO - utils - Saved list with one item per line to: data/captions-analyze-samples-outputs/slop_list.json\n",
      "2025-04-14 14:42:41,878 - INFO - slop_lists - Saved standard word slop list (1500 words).\n",
      "2025-04-14 14:42:41,881 - INFO - slop_lists - Saved frequency-sorted word slop list (1500 words).\n",
      "2025-04-14 14:42:41,881 - INFO - slop_lists - Aggregating N-gram data for slop lists...\n",
      "Aggregating N-grams: 100%|███████████████████████| 3/3 [00:00<00:00, 569.67it/s]\n",
      "2025-04-14 14:42:41,888 - INFO - utils - Saved list with one item per line to: data/captions-analyze-samples-outputs/slop_list_bigrams.json\n",
      "2025-04-14 14:42:41,888 - INFO - slop_lists - Saved bigram slop list (153 bigrams).\n",
      "2025-04-14 14:42:41,888 - INFO - utils - Saved list with one item per line to: data/captions-analyze-samples-outputs/slop_list_trigrams.json\n",
      "2025-04-14 14:42:41,888 - INFO - slop_lists - Saved trigram slop list (127 trigrams).\n",
      "2025-04-14 14:42:41,888 - INFO - slop_lists - Extracting and saving slop phrases from combined data...\n",
      "2025-04-14 14:42:41,888 - INFO - slop_lists - Extracting top 1000 3-grams, then retrieving phrases...\n",
      "2025-04-14 14:42:41,888 - INFO - slop_lists - Extracting cleaned 3-grams from 30000 combined texts...\n",
      "2025-04-14 14:43:24,810 - INFO - slop_lists - Found 1000 unique 3-grams after cleaning.\n",
      "2025-04-14 14:43:24,810 - INFO - slop_lists - Created set of 1000 top n-gram tuples.\n",
      "2025-04-14 14:43:24,810 - INFO - slop_lists - Spawning up to 8 worker processes for phrase extraction...\n",
      "MP substring extraction: 100%|██████████| 30000/30000 [00:10<00:00, 2878.43it/s]\n",
      "2025-04-14 14:43:35,413 - INFO - slop_lists - Merged counters: 6274 unique substrings found.\n",
      "2025-04-14 14:43:35,417 - INFO - slop_lists - After filtering, we have 5700 unique phrases.\n",
      "2025-04-14 14:43:35,451 - INFO - slop_lists - Saved phrase data to: data/captions-analyze-samples-outputs/slop_list_phrases.jsonl\n",
      "2025-04-14 14:43:35,451 - INFO - slop_lists - Saved top 5700 phrases to data/captions-analyze-samples-outputs/slop_list_phrases.jsonl.\n",
      "2025-04-14 14:43:35,463 - INFO - slop_lists - Slop list + phrase generation finished.\n",
      "2025-04-14 14:43:35,475 - INFO - create_slop_lists - Slop list creation script finished.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_slop_lists.py --output-dir data/captions-analyze-samples-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7beb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY not found in environment variables or .env file.\n",
      "2025-04-14 14:44:38,224 - INFO - generate_phylo_trees - Starting phylogenetic tree generation using data from: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 14:44:38,224 - INFO - generate_phylo_trees - Output directory: data/captions-analyze-samples-outputs\n",
      "2025-04-14 14:44:38,224 - INFO - generate_phylo_trees - Top N features per model: 1500\n",
      "2025-04-14 14:44:38,224 - INFO - phylogeny - Starting phylogenetic tree generation...\n",
      "2025-04-14 14:44:38,224 - INFO - phylogeny - Loading combined metrics data from: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 14:44:38,229 - INFO - phylogeny - Extracting features (top words/ngrams) for tree building...\n",
      "2025-04-14 14:44:38,230 - INFO - phylogeny - Attempting parsimony tree construction using PHYLIP...\n",
      "2025-04-14 14:44:38,231 - INFO - phylogeny - Parsimony analysis: 3 models, 2419 features.\n",
      "2025-04-14 14:44:38,233 - ERROR - phylogeny - Could not find PHYLIP 'pars' executable in system PATH or configured PHYLIP_PATH.\n",
      "2025-04-14 14:44:38,233 - WARNING - phylogeny - Parsimony tree failed or was skipped. Attempting hierarchical clustering fallback...\n",
      "2025-04-14 14:44:38,233 - INFO - phylogeny - Building tree using fallback hierarchical clustering (SciPy)...\n",
      "2025-04-14 14:44:38,234 - INFO - phylogeny - Hierarchical clustering: 3 models, 2419 features.\n",
      "2025-04-14 14:44:38,413 - INFO - phylogeny - Successfully built hierarchical clustering tree.\n",
      "2025-04-14 14:44:38,414 - INFO - phylogeny - Successfully generated hierarchical clustering tree.\n",
      "2025-04-14 14:44:38,414 - INFO - phylogeny - Rendering final hierarchical tree visualizations...\n",
      "2025-04-14 14:44:38,627 - INFO - phylogeny - Saved basic overview tree: data/captions-analyze-samples-outputs/hierarchical_tree_basic.png\n",
      "Rendering hierarchical charts:   0%|                      | 0/3 [00:00<?, ?it/s]2025-04-14 14:44:38,687 - INFO - phylogeny - Saved C tree 'auto-caption__captioner-1.4__200words__hierarchical_circular.png' (highlight: auto-caption__captioner-1.4__200words)\n",
      "2025-04-14 14:44:38,692 - INFO - phylogeny - Saved R tree 'auto-caption__captioner-1.4__200words__hierarchical_rectangular.png' (highlight: auto-caption__captioner-1.4__200words)\n",
      "2025-04-14 14:44:38,747 - INFO - phylogeny - Saved C tree 'auto-caption__captioner-1.45__200words__hierarchical_circular.png' (highlight: auto-caption__captioner-1.45__200words)\n",
      "2025-04-14 14:44:38,752 - INFO - phylogeny - Saved R tree 'auto-caption__captioner-1.45__200words__hierarchical_rectangular.png' (highlight: auto-caption__captioner-1.45__200words)\n",
      "Rendering hierarchical charts:  67%|█████████▎    | 2/3 [00:00<00:00, 16.04it/s]2025-04-14 14:44:38,806 - INFO - phylogeny - Saved C tree 'auto-caption__captioner-1.45__full__hierarchical_circular.png' (highlight: auto-caption__captioner-1.45__full)\n",
      "2025-04-14 14:44:38,811 - INFO - phylogeny - Saved R tree 'auto-caption__captioner-1.45__full__hierarchical_rectangular.png' (highlight: auto-caption__captioner-1.45__full)\n",
      "Rendering hierarchical charts: 100%|██████████████| 3/3 [00:00<00:00, 16.31it/s]\n",
      "2025-04-14 14:44:38,812 - INFO - phylogeny - Saved tree in Newick format: data/captions-analyze-samples-outputs/hierarchical_tree.nwk\n",
      "2025-04-14 14:44:38,812 - INFO - phylogeny - Saved tree in Nexus format: data/captions-analyze-samples-outputs/hierarchical_tree.nex\n",
      "2025-04-14 14:44:38,812 - INFO - phylogeny - Phylogenetic tree generation (hierarchical) completed successfully.\n",
      "2025-04-14 14:44:38,812 - INFO - generate_phylo_trees - Phylogenetic tree generation script finished.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/generate_phylo_trees.py --output-dir data/captions-analyze-samples-outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
