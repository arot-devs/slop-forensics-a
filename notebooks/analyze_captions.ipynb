{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c929ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global temporary directory: /tmp/unibox_temp\n",
      "\u001b[37m2025-04-14 13:11:18 [INFO] loads: Loading from hf://incantor/danbooru_mongodb-quail_dump\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11a4d17060541ada48dbc2ad3e3d9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48647855e33a47bf9ad80bc6a53a2b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import unibox as ub\n",
    "\n",
    "\n",
    "dset = ub.loads(\"hf://incantor/danbooru_mongodb-quail_dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rows = [\n",
    "    \"filepath\",\n",
    "    \"auto-caption__captioner-1.45__full\",\n",
    "    \"auto-caption__captioner-1.45__200words\",\n",
    "    \"auto-caption__captioner-1.4__200words\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66aadd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3de325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248f9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Run this in a Python interpreter or script\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('cmudict', quiet=True)\n",
    "print(\"NLTK data downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d03fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '.env.example': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp .env.example .env\n",
    "# You can leave the default values or add your PHYLIP_PATH if needed later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a4abe",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "- **Goal:** Convert your DataFrame into .jsonl files, one file per language model. Each line in the file should be a JSON object representing one input-output pair.\n",
    "\n",
    "- **Format:** The analysis script (slop_profile.py) expects each JSON object to have at least an \"output\" key. For best results and compatibility with all features (like multi-prompt n-gram analysis), include:\n",
    "\n",
    "    - \"output\": (Required) The raw text generated by the LLM.\n",
    "    - \"model\": (Highly Recommended) The identifier for the model (e.g., \"meta-llama/Llama-3-8b-chat-hf\"). If this is missing, the script *must* infer it from the filename.\n",
    "    - \"prompt\": (Recommended) The original input prompt.\n",
    "    - \"source\": (Recommended) A category or source name for the prompt (e.g., \"my_dataset\").\n",
    "    - \"id\": (Recommended) A unique identifier for the prompt within its source (e.g., row index or a specific ID).\n",
    "\n",
    "- **Filename Convention:** Name your files like generated_{provider}__{model_name}.jsonl. Use the utils.sanitize_filename logic (replace / with __, remove invalid chars). Example: generated_meta-llama__Llama-3-8b-chat-hf.jsonl. This is crucial if the \"model\" key isn't in your JSON objects.\n",
    "\n",
    "- **Create an Input Directory:** Make a directory to hold these files, for example: mkdir my_analysis_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa897ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: model_A\n",
      "  Successfully wrote 2 records to my_analysis_input/generated_model_A.jsonl\n",
      "Processing model: model_B\n",
      "  Successfully wrote 2 records to my_analysis_input/generated_model_B.jsonl\n",
      "\n",
      "Data preparation finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assume df is your loaded DataFrame with columns 'input_prompt', 'llm_output', 'model_name'\n",
    "# Example DataFrame creation (replace with your actual data loading):\n",
    "data = {\n",
    "    'input_prompt': [\"Write a story about a cat.\", \"Describe a sunset.\", \"Write a story about a dog.\", \"Explain gravity.\"],\n",
    "    'llm_output': [\"The cat sat lazily. It dreamed of chasing mice.\", \"The sun dipped low, painting the sky orange and purple.\", \"Buddy barked happily, wagging his tail.\", \"Gravity is the force pulling objects together.\"],\n",
    "    'model_name': [\"model_A\", \"model_A\", \"model_B\", \"model_B\"] # Example model names\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_directory = \"my_analysis_input\"\n",
    "prompt_source_name = \"my_custom_prompts\" # Name your prompt source\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Make sure the slop_forensics library can be imported for sanitize_filename\n",
    "# Add the parent directory of 'slop_forensics' package to the path\n",
    "script_dir = \"../\"\n",
    "project_root = os.path.dirname(script_dir) # Assumes script is in repo root or similar\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    from slop_forensics.utils import sanitize_filename\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import sanitize_filename. Make sure you are running this script\")\n",
    "    print(f\"from a location where the 'slop_forensics' package is accessible (e.g., repo root),\")\n",
    "    print(f\"or add the project root ('{project_root}') to your PYTHONPATH.\")\n",
    "    # Basic fallback sanitizer if import fails\n",
    "    def sanitize_filename(name: str) -> str:\n",
    "        sanitized = name.replace(\"/\", \"__\")\n",
    "        sanitized = re.sub(r'[<>:\"|?*\\\\ ]', '-', sanitized)\n",
    "        sanitized = sanitized.strip('-_')\n",
    "        return sanitized if sanitized else \"invalid_name\"\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Group data by model\n",
    "grouped = df.groupby('model_name')\n",
    "\n",
    "for model_name, group in grouped:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    sanitized_model_name = sanitize_filename(model_name)\n",
    "    output_filename = os.path.join(output_directory, f\"generated_{sanitized_model_name}.jsonl\")\n",
    "\n",
    "    records = []\n",
    "    for index, row in group.iterrows():\n",
    "        record = {\n",
    "            \"source\": prompt_source_name,\n",
    "            \"id\": str(index), # Use DataFrame index as unique ID within the source\n",
    "            \"prompt\": row.get('input_prompt', ''), # Handle potential missing column\n",
    "            \"model\": model_name,\n",
    "            \"output\": row.get('llm_output', '') # Handle potential missing column\n",
    "        }\n",
    "        # Ensure output is a non-empty string\n",
    "        if not isinstance(record[\"output\"], str) or not record[\"output\"].strip():\n",
    "            print(f\"  Skipping row {index} for model {model_name} due to empty or invalid output.\")\n",
    "            continue\n",
    "        records.append(record)\n",
    "\n",
    "    if not records:\n",
    "        print(f\"  No valid records found for model {model_name}. Skipping file creation.\")\n",
    "        continue\n",
    "\n",
    "    # Write to JSONL file\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for record in records:\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "        print(f\"  Successfully wrote {len(records)} records to {output_filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"  Error writing file {output_filename}: {e}\")\n",
    "    except TypeError as e:\n",
    "        print(f\"  Error serializing record to JSON for {output_filename}: {e}\")\n",
    "\n",
    "print(\"\\nData preparation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a824011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY not found in environment variables or .env file.\n",
      "2025-04-14 13:50:50,215 - INFO - slop_profile - Starting analysis of datasets in: my_analysis_input\n",
      "2025-04-14 13:50:50,216 - INFO - slop_profile - Analysis output directory: /home/ubuntu/dev/slop-forensics-a/results/analysis\n",
      "2025-04-14 13:50:50,216 - INFO - slop_profile - Combined metrics output file: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 13:50:50,216 - INFO - slop_profile - Max items per model: 10000\n",
      "2025-04-14 13:50:50,216 - INFO - slop_profile - Will log top 5 patterns per model\n",
      "2025-04-14 13:50:50,216 - INFO - slop_profile - Found 2 dataset files to analyze.\n",
      "2025-04-14 13:50:50,252 - INFO - slop_profile - Loaded existing combined data for 38 models from /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "Analyzing Models:   0%|                                   | 0/2 [00:00<?, ?it/s]2025-04-14 13:50:50,253 - INFO - slop_profile - Processing file: generated_model_A.jsonl\n",
      "2025-04-14 13:50:50,254 - INFO - slop_profile - Analyzing model: model_A (2 items)\n",
      "2025-04-14 13:50:50,254 - INFO - analysis - Starting analysis for model: model_A\n",
      "2025-04-14 13:50:51,359 - WARNING - metrics - Slop file for type 'word' not found at data/slop_list.json. Returning empty set.\n",
      "2025-04-14 13:50:51,359 - WARNING - metrics - Slop file for type 'bigram' not found at data/slop_list_bigrams.json. Returning empty set.\n",
      "2025-04-14 13:50:51,359 - WARNING - metrics - Slop file for type 'trigram' not found at data/slop_list_trigrams.json. Returning empty set.\n",
      "2025-04-14 13:50:51,359 - WARNING - metrics - No slop lists loaded. Returning slop index 0.\n",
      "2025-04-14 13:50:51,359 - WARNING - analysis - No words remained after filtering for model_A. Skipping rarity and repetition analysis.\n",
      "2025-04-14 13:50:51,360 - INFO - analysis - Analysis complete for model: model_A\n",
      "Analyzing Models:  50%|█████████████▌             | 1/2 [00:01<00:01,  1.11s/it]2025-04-14 13:50:51,360 - INFO - slop_profile - Processing file: generated_model_B.jsonl\n",
      "2025-04-14 13:50:51,360 - INFO - slop_profile - Analyzing model: model_B (2 items)\n",
      "2025-04-14 13:50:51,360 - INFO - analysis - Starting analysis for model: model_B\n",
      "2025-04-14 13:50:51,361 - WARNING - metrics - No slop lists loaded. Returning slop index 0.\n",
      "2025-04-14 13:50:51,361 - WARNING - analysis - No words remained after filtering for model_B. Skipping rarity and repetition analysis.\n",
      "2025-04-14 13:50:51,361 - INFO - analysis - Analysis complete for model: model_B\n",
      "Analyzing Models: 100%|███████████████████████████| 2/2 [00:01<00:00,  1.81it/s]\n",
      "2025-04-14 13:50:51,361 - INFO - slop_profile - Saving combined metrics for 40 models to /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - \n",
      "========== SUMMARY OF TOP PATTERNS ==========\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - MODEL: model_A\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - WORDS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - BIGRAMS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - TRIGRAMS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - ---\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - MODEL: model_B\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - WORDS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - BIGRAMS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - TRIGRAMS: None found\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - ---\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - ============== END SUMMARY ===============\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - Analysis script finished.\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - \n",
      "Full results are available at:\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - - Combined metrics file: /home/ubuntu/dev/slop-forensics-a/results/slop_profile_results.json\n",
      "2025-04-14 13:50:51,566 - INFO - slop_profile - - Individual analysis files: /home/ubuntu/dev/slop-forensics-a/results/analysis/analysis_*.json\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/slop_profile.py --input-dir my_analysis_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1cab4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY not found in environment variables or .env file.\n",
      "2025-04-14 13:51:12,002 - INFO - create_slop_lists - Starting slop list creation from analysis files in: /home/ubuntu/dev/slop-forensics-a/results/analysis\n",
      "2025-04-14 13:51:12,002 - INFO - create_slop_lists - Output directory: /home/ubuntu/dev/slop-forensics-a/results/slop_lists\n",
      "2025-04-14 13:51:12,002 - INFO - slop_lists - Starting combined slop list generation...\n",
      "2025-04-14 13:51:12,002 - INFO - slop_lists - Found 2 analysis files. Loading data...\n",
      "Loading analysis files:   0%|                             | 0/2 [00:00<?, ?it/s]2025-04-14 13:51:12,003 - WARNING - slop_lists - Dataset file not found for model_B: /home/ubuntu/dev/slop-forensics-a/results/datasets/generated_model_B.jsonl\n",
      "2025-04-14 13:51:12,003 - WARNING - slop_lists - Dataset file not found for model_A: /home/ubuntu/dev/slop-forensics-a/results/datasets/generated_model_A.jsonl\n",
      "Loading analysis files: 100%|███████████████████| 2/2 [00:00<00:00, 5246.16it/s]\n",
      "2025-04-14 13:51:12,003 - ERROR - slop_lists - No valid model data loaded. Cannot create slop lists.\n",
      "2025-04-14 13:51:12,003 - INFO - create_slop_lists - Slop list creation script finished.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/create_slop_lists.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
